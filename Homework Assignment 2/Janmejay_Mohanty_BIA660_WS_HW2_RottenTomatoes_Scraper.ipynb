{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NAME: JANMEJAY MOHANTY <br> COURSE: BIA 660 WS <br> COURSE NAME: WEB MINING <br> CWID: 20009315 <br> EMAIL: JMOHANTY@STEVENS.EDU <br> PROGRAM: ROTTEN TOMATOES SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv,re,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all the review data from the current page and writes to the specified writer\n",
    "def get_data(writer:csv.writer):\n",
    "\n",
    "    reviews = driver.find_elements(by=By.CSS_SELECTOR,value='div[class=\"row review_table_row\"]')    #get all review divs in the current page\n",
    "\n",
    "    for review in reviews:  # for each review div\n",
    "       \n",
    "        reviewer_name,review_date,review_text,review_polarity='NA','NA','NA','NA' # initialize critic, text, date, platform and rating\n",
    "        \n",
    "        try:\n",
    "            # try to get the reviewer name\n",
    "            reviewer_name_chunk=review.find_element(by=By.CSS_SELECTOR,value='a')   # get the reviewer name box\n",
    "            reviewer_name=reviewer_name_chunk.text   # get the reviewer name string\n",
    "\n",
    "        except NoSuchElementException as e: # reviewer name could not be found\n",
    "            print('could not extract reviewer name')    \n",
    "        \n",
    "        try:    \n",
    "            # try to get the date\n",
    "            date_box=review.find_element(by=By.CSS_SELECTOR,value='div[class=\"review-date subtle small\"]')  # get the date box\n",
    "            review_date=date_box.text   # get the date string\n",
    "\n",
    "        except NoSuchElementException as e: # review date could not be found\n",
    "            print('could not extract review date')\n",
    "\n",
    "        try:\n",
    "            # try to get the review text\n",
    "            review_text_chunk=review.find_element(by=By.CSS_SELECTOR,value='div[class=\"the_review\"]')   # get the review text box\n",
    "            review_text=review_text_chunk.text  # get the review text box string\n",
    "        \n",
    "        except NoSuchElementException as e: # review text could not be found\n",
    "             print('could not extract review text')   \n",
    "        \n",
    "        try:\n",
    "            # try to get the review polarity\n",
    "            review_polarity_chunk = review.find_element(by=By.CSS_SELECTOR,value='div[class*=\"review_icon icon small\"]')   # get the review polarity box\n",
    "            review_polarity_value = review_polarity_chunk.get_attribute('class')\n",
    "            if (review_polarity_value.find('rotten')>0):\n",
    "                review_polarity = 'rotten'\n",
    "            if (review_polarity_value.find('fresh')>0):\n",
    "                review_polarity= 'fresh'  \n",
    "\n",
    "        except NoSuchElementException as e: # review polarity could not be found\n",
    "            print('could not extract review polarity')\n",
    "\n",
    "        writer.writerow([reviewer_name,review_date,review_text,review_polarity]) # write a new row to file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(driver,url):\n",
    "    delay:int = 5   # number of seconds to wait \n",
    "\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)  # visit the page\n",
    "\n",
    "    input() # wait until we can click on the cookie overlay\n",
    "\n",
    "    page_count = 1  # keep track of page count\n",
    "\n",
    "    # create a new csv writer for the review data\n",
    "    fw=open('Output.csv','w',encoding=\"utf-8\")  # output file\n",
    "\n",
    "    writer=csv.writer(fw,lineterminator='\\n')   # create a csv writer for this file\n",
    "    \n",
    "    writer.writerow([\"reviewer_name\", \"review date\", \"review text\", \"review_polarity\"])\n",
    "\n",
    "    while True: # keep going until there are no more pages\n",
    "\n",
    "        print(\"Page No: \",page_count)   # print current page count\n",
    "\n",
    "        page_count+=1   # increment page count\n",
    "\n",
    "        get_data(writer)    # extract and write the review data from the current page\n",
    "        \n",
    "        try:\n",
    "            # wait until the next button appears\n",
    "            next_button = WebDriverWait(driver, delay).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[class*=\"js-prev-next-paging-next btn prev-next-paging__button prev-next-paging__button-right\"]')))\n",
    "        \n",
    "        except TimeoutException:\n",
    "            break\n",
    "\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "    fw.close()    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rottentomatoes.com/m/tenet/reviews'\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page No:  1\n",
      "Page No:  2\n",
      "Page No:  3\n",
      "Page No:  4\n",
      "Page No:  5\n",
      "Page No:  6\n",
      "Page No:  7\n",
      "Page No:  8\n",
      "Page No:  9\n",
      "Page No:  10\n",
      "Page No:  11\n",
      "Page No:  12\n",
      "Page No:  13\n",
      "Page No:  14\n",
      "Page No:  15\n",
      "Page No:  16\n",
      "Page No:  17\n",
      "Page No:  18\n",
      "Page No:  19\n"
     ]
    }
   ],
   "source": [
    "scrape(driver,url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79f58b2da1d0ebeef0843b536b913fe6ecf5d6027e7f201d25bafe70c90b7910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
